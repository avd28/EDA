{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EDA + Logistic Regression + PCA**\n",
    "\n",
    "\n",
    "Hello friends,\n",
    "\n",
    "This kernel is all about **Principal Component Analysis** - a **Dimensionality Reduction** technique.\n",
    "\n",
    "I have discussed **Principal Component Analysis (PCA)**. In particular, I have introduced PCA, explained variance ratio, Logistic Regression with PCA, find right number of dimensions and plotting explained variance ratio with number of dimensions.\n",
    "\n",
    "I have used the **adult** data set for this kernel. This dataset is very small for PCA purpose. My main purpose is to demonstrate PCA implementation with this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I hope you find this kernel useful and your <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "The contents of this kernel is divided into various topics which are as follows:-\n",
    "\n",
    "-   The Curse of Dimensionality\n",
    "-\tIntroduction to Principal Component Analysis\n",
    "-\tImport Python libraries\n",
    "-\tImport dataset\n",
    "-\tExploratory data analysis\n",
    "-\tSplit data into training and test set\n",
    "-\tFeature engineering\n",
    "-\tFeature scaling\n",
    "-\tLogistic regression model with all features\n",
    "-\tLogistic Regression with PCA\n",
    "-\tSelect right number of dimensions\n",
    "-\tPlot explained variance ratio with number of dimensions\n",
    "-\tConclusion\n",
    "-\tReferences\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "\n",
    "Generally, real world datasets contain thousands or millions of features to train for. This is very time consuming task as this makes training extremely slow. In such cases, it is very difficult to find a good solution. This problem is often referred to as the curse of dimensionality.\n",
    "\n",
    "\n",
    "**The curse of dimensionality** refers to various phenomena that arise when we analyze and organize data in high dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings. The problem is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires statistical significance.\n",
    "\n",
    "\n",
    "In real-world problems, it is often possible to reduce the number of dimensions considerably. This process is called **dimensionality reduction**. It refers to the process of reducing the number of dimensions under consideration by obtaining a set of principal variables. It helps to speed up training and is also extremely useful for data visualization.\n",
    "\n",
    "\n",
    "The most popular dimensionality reduction technique is Principal Component Analysis (PCA), which is discussed below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Principal Component Analysis (PCA)\n",
    "\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique that can be used to reduce a larger set of feature variables into a smaller set that still contains most of the variance in the larger set. \n",
    "\n",
    "### Preserve the variance\n",
    "\n",
    "PCA, first identifies the hyperplane that lies closest to the data and then it projects the data onto it. Before, we can project the training set onto a lower-dimensional hyperplane, we need to select the right hyperplane. The projection can be done in such a way so as to preserve the maximum variance. This is the idea behind PCA.\n",
    "\n",
    "### Principal Components\n",
    "\n",
    "PCA identifies the axes that accounts for the maximum amount of cumulative sum of variance in the training set. These are called Principal Components. PCA assumes that the dataset is centered around the origin. Scikit-Learn’s PCA classes take care of centering the data automatically.\n",
    "\n",
    "### Projecting down to d Dimensions\n",
    "\n",
    "Once, we have identified all the principal components, we can reduce the dimensionality of the dataset down to d dimensions by projecting it onto the hyperplane defined by the first d principal components. This ensures that the projection will preserve as much variance as possible.\n",
    "\n",
    "\n",
    "\n",
    "Now, let's get to the implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adult.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "# import libraries for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "\n",
    "# Working with os module - os is a module in Python 3.\n",
    "# Its main purpose is to interact with the operating system. \n",
    "# It provides functionalities to manipulate files and folders.\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# File sizes\n",
      "adult.csv                     4.1MB\n"
     ]
    }
   ],
   "source": [
    "print('# File sizes')\n",
    "for f in os.listdir('../input'):\n",
    "    print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + 'MB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 85.2 ms, sys: 23.7 ms, total: 109 ms\n",
      "Wall time: 114 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "file = ('../input/adult.csv')\n",
    "df = pd.read_csv(file, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check shape of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 32561 instances and 15 attributes in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>?</td>\n",
       "      <td>77053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>?</td>\n",
       "      <td>186061</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>140359</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
       "0   90         ?   77053       HS-grad              9        Widowed   \n",
       "1   82   Private  132870       HS-grad              9        Widowed   \n",
       "2   66         ?  186061  Some-college             10        Widowed   \n",
       "3   54   Private  140359       7th-8th              4       Divorced   \n",
       "4   41   Private  264663  Some-college             10      Separated   \n",
       "\n",
       "          occupation   relationship   race     sex  capital.gain  \\\n",
       "0                  ?  Not-in-family  White  Female             0   \n",
       "1    Exec-managerial  Not-in-family  White  Female             0   \n",
       "2                  ?      Unmarried  Black  Female             0   \n",
       "3  Machine-op-inspct      Unmarried  White  Female             0   \n",
       "4     Prof-specialty      Own-child  White  Female             0   \n",
       "\n",
       "   capital.loss  hours.per.week native.country income  \n",
       "0          4356              40  United-States  <=50K  \n",
       "1          4356              18  United-States  <=50K  \n",
       "2          4356              40  United-States  <=50K  \n",
       "3          3900              40  United-States  <=50K  \n",
       "4          3900              40  United-States  <=50K  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View summary of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      "age               32561 non-null int64\n",
      "workclass         32561 non-null object\n",
      "fnlwgt            32561 non-null int64\n",
      "education         32561 non-null object\n",
      "education.num     32561 non-null int64\n",
      "marital.status    32561 non-null object\n",
      "occupation        32561 non-null object\n",
      "relationship      32561 non-null object\n",
      "race              32561 non-null object\n",
      "sex               32561 non-null object\n",
      "capital.gain      32561 non-null int64\n",
      "capital.loss      32561 non-null int64\n",
      "hours.per.week    32561 non-null int64\n",
      "native.country    32561 non-null object\n",
      "income            32561 non-null object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Summary of the dataset shows that there are no missing values. But the preview shows that the dataset contains values coded as `?`. So, I will encode `?` as NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode `?` as `NaNs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df == '?'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again check the summary of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      "age               32561 non-null int64\n",
      "workclass         30725 non-null object\n",
      "fnlwgt            32561 non-null int64\n",
      "education         32561 non-null object\n",
      "education.num     32561 non-null int64\n",
      "marital.status    32561 non-null object\n",
      "occupation        30718 non-null object\n",
      "relationship      32561 non-null object\n",
      "race              32561 non-null object\n",
      "sex               32561 non-null object\n",
      "capital.gain      32561 non-null int64\n",
      "capital.loss      32561 non-null int64\n",
      "hours.per.week    32561 non-null int64\n",
      "native.country    31978 non-null object\n",
      "income            32561 non-null object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the summary shows that the variables - `workclass`, `occupation` and `native.country` contain missing values. All of these variables are categorical data type. So, I will impute the missing values with the most frequent value- the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values with mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['workclass', 'occupation', 'native.country']:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check again for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "workclass         0\n",
       "fnlwgt            0\n",
       "education         0\n",
       "education.num     0\n",
       "marital.status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "sex               0\n",
       "capital.gain      0\n",
       "capital.loss      0\n",
       "hours.per.week    0\n",
       "native.country    0\n",
       "income            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that there are no missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting feature vector and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['income'], axis=1)\n",
    "\n",
    "y = df['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>Private</td>\n",
       "      <td>77053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>Private</td>\n",
       "      <td>186061</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>140359</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
       "0   90   Private   77053       HS-grad              9        Widowed   \n",
       "1   82   Private  132870       HS-grad              9        Widowed   \n",
       "2   66   Private  186061  Some-college             10        Widowed   \n",
       "3   54   Private  140359       7th-8th              4       Divorced   \n",
       "4   41   Private  264663  Some-college             10      Separated   \n",
       "\n",
       "          occupation   relationship   race     sex  capital.gain  \\\n",
       "0     Prof-specialty  Not-in-family  White  Female             0   \n",
       "1    Exec-managerial  Not-in-family  White  Female             0   \n",
       "2     Prof-specialty      Unmarried  Black  Female             0   \n",
       "3  Machine-op-inspct      Unmarried  White  Female             0   \n",
       "4     Prof-specialty      Own-child  White  Female             0   \n",
       "\n",
       "   capital.loss  hours.per.week native.country  \n",
       "0          4356              40  United-States  \n",
       "1          4356              18  United-States  \n",
       "2          4356              40  United-States  \n",
       "3          3900              40  United-States  \n",
       "4          3900              40  United-States  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into separate training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "categorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
    "for feature in categorical:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[feature] = le.fit_transform(X_train[feature])\n",
    "        X_test[feature] = le.transform(X_test[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.101484</td>\n",
       "      <td>2.600478</td>\n",
       "      <td>-1.494279</td>\n",
       "      <td>-0.332263</td>\n",
       "      <td>1.133894</td>\n",
       "      <td>-0.402341</td>\n",
       "      <td>-0.782234</td>\n",
       "      <td>2.214196</td>\n",
       "      <td>0.39298</td>\n",
       "      <td>-1.430470</td>\n",
       "      <td>-0.145189</td>\n",
       "      <td>-0.217407</td>\n",
       "      <td>-1.662414</td>\n",
       "      <td>0.262317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028248</td>\n",
       "      <td>-1.884720</td>\n",
       "      <td>0.438778</td>\n",
       "      <td>0.184396</td>\n",
       "      <td>-0.423425</td>\n",
       "      <td>-0.402341</td>\n",
       "      <td>-0.026696</td>\n",
       "      <td>-0.899410</td>\n",
       "      <td>0.39298</td>\n",
       "      <td>0.699071</td>\n",
       "      <td>-0.145189</td>\n",
       "      <td>-0.217407</td>\n",
       "      <td>-0.200753</td>\n",
       "      <td>0.262317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.247956</td>\n",
       "      <td>-0.090641</td>\n",
       "      <td>0.045292</td>\n",
       "      <td>1.217715</td>\n",
       "      <td>-0.034095</td>\n",
       "      <td>0.926666</td>\n",
       "      <td>-0.782234</td>\n",
       "      <td>-0.276689</td>\n",
       "      <td>0.39298</td>\n",
       "      <td>-1.430470</td>\n",
       "      <td>-0.145189</td>\n",
       "      <td>-0.217407</td>\n",
       "      <td>-0.038346</td>\n",
       "      <td>0.262317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.850587</td>\n",
       "      <td>-1.884720</td>\n",
       "      <td>0.793152</td>\n",
       "      <td>0.184396</td>\n",
       "      <td>-0.423425</td>\n",
       "      <td>0.926666</td>\n",
       "      <td>-0.530388</td>\n",
       "      <td>0.968753</td>\n",
       "      <td>0.39298</td>\n",
       "      <td>0.699071</td>\n",
       "      <td>-0.145189</td>\n",
       "      <td>-0.217407</td>\n",
       "      <td>-0.038346</td>\n",
       "      <td>0.262317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.044989</td>\n",
       "      <td>-2.781760</td>\n",
       "      <td>-0.853275</td>\n",
       "      <td>0.442726</td>\n",
       "      <td>1.523223</td>\n",
       "      <td>-0.402341</td>\n",
       "      <td>-0.782234</td>\n",
       "      <td>-0.899410</td>\n",
       "      <td>0.39298</td>\n",
       "      <td>0.699071</td>\n",
       "      <td>-0.145189</td>\n",
       "      <td>-0.217407</td>\n",
       "      <td>-0.038346</td>\n",
       "      <td>0.262317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  workclass    fnlwgt  education  education.num  marital.status  \\\n",
       "0  0.101484   2.600478 -1.494279  -0.332263       1.133894       -0.402341   \n",
       "1  0.028248  -1.884720  0.438778   0.184396      -0.423425       -0.402341   \n",
       "2  0.247956  -0.090641  0.045292   1.217715      -0.034095        0.926666   \n",
       "3 -0.850587  -1.884720  0.793152   0.184396      -0.423425        0.926666   \n",
       "4 -0.044989  -2.781760 -0.853275   0.442726       1.523223       -0.402341   \n",
       "\n",
       "   occupation  relationship     race       sex  capital.gain  capital.loss  \\\n",
       "0   -0.782234      2.214196  0.39298 -1.430470     -0.145189     -0.217407   \n",
       "1   -0.026696     -0.899410  0.39298  0.699071     -0.145189     -0.217407   \n",
       "2   -0.782234     -0.276689  0.39298 -1.430470     -0.145189     -0.217407   \n",
       "3   -0.530388      0.968753  0.39298  0.699071     -0.145189     -0.217407   \n",
       "4   -0.782234     -0.899410  0.39298  0.699071     -0.145189     -0.217407   \n",
       "\n",
       "   hours.per.week  native.country  \n",
       "0       -1.662414        0.262317  \n",
       "1       -0.200753        0.262317  \n",
       "2       -0.038346        0.262317  \n",
       "3       -0.038346        0.262317  \n",
       "4       -0.038346        0.262317  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy score with all the features: 0.8218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Logistic Regression accuracy score with all the features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with PCA\n",
    "\n",
    "Scikit-Learn's PCA class implements PCA algorithm using the code below. Before diving deep, I will explain another important concept called explained variance ratio.\n",
    "\n",
    "\n",
    "### Explained Variance Ratio\n",
    "\n",
    "A very useful piece of information is the **explained variance ratio** of each principal component. It is available via the `explained_variance_ratio_ ` variable. It indicates the proportion of the dataset’s variance that lies along the axis of each principal component.\n",
    "\n",
    "Now, let's get to the PCA implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14757168, 0.10182915, 0.08147199, 0.07880174, 0.07463545,\n",
       "       0.07274281, 0.07009602, 0.06750902, 0.0647268 , 0.06131155,\n",
       "       0.06084207, 0.04839584, 0.04265038, 0.02741548])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "X_train = pca.fit_transform(X_train)\n",
    "pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- We can see that approximately 97.25% of variance is explained by the first 13 variables. \n",
    "\n",
    "- Only 2.75% of variance is explained by the last variable. So, we can assume that it carries little information. \n",
    "\n",
    "- So, I will drop it, train the model again and calculate the accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with first 13 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy score with the first 13 features: 0.8213\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(['income','native.country'], axis=1)\n",
    "y = df['income']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "categorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex']\n",
    "for feature in categorical:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[feature] = le.fit_transform(X_train[feature])\n",
    "        X_test[feature] = le.transform(X_test[feature])\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Logistic Regression accuracy score with the first 13 features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- We can see that accuracy has been decreased from 0.8218 to 0.8213 after dropping the last feature.\n",
    "\n",
    "- Now, if I take the last two features combined, then we can see that approximately 7% of variance is explained by them.\n",
    "\n",
    "- I will drop them, train the model again and calculate the accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with first 12 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy score with the first 12 features: 0.8227\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(['income','native.country', 'hours.per.week'], axis=1)\n",
    "y = df['income']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "categorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex']\n",
    "for feature in categorical:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[feature] = le.fit_transform(X_train[feature])\n",
    "        X_test[feature] = le.transform(X_test[feature])\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Logistic Regression accuracy score with the first 12 features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- Now, it can be seen that the accuracy has been increased to 0.8227, if the model is trained with 12 features.\n",
    "\n",
    "- Lastly, I will take the last three features combined. Approximately 11.83% of variance is explained by them.\n",
    "\n",
    "- I will repeat the process, drop these features, train the model again and calculate the accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with first 11 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy score with the first 11 features: 0.8187\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(['income','native.country', 'hours.per.week', 'capital.loss'], axis=1)\n",
    "y = df['income']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "categorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex']\n",
    "for feature in categorical:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[feature] = le.fit_transform(X_train[feature])\n",
    "        X_test[feature] = le.transform(X_test[feature])\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Logistic Regression accuracy score with the first 11 features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- We can see that accuracy has significantly decreased to 0.8187 if I drop the last three features.\n",
    "\n",
    "- Our aim is to maximize the accuracy. We get maximum accuracy with the first 12 features and the accuracy is 0.8227."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select right number of dimensions\n",
    "\n",
    "- The above process works well if the number of dimensions are small.\n",
    "\n",
    "- But, it is quite cumbersome if we have large number of dimensions.\n",
    "\n",
    "- In that case, a better approach is to compute the number of dimensions that can explain significantly large portion of the variance.\n",
    "\n",
    "- The following code computes PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 90% of the training set variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of dimensions required to preserve 90% of variance is 12\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(['income'], axis=1)\n",
    "y = df['income']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "categorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
    "for feature in categorical:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[feature] = le.fit_transform(X_train[feature])\n",
    "        X_test[feature] = le.transform(X_test[feature])\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "\n",
    "\n",
    "pca= PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "dim = np.argmax(cumsum >= 0.90) + 1\n",
    "print('The number of dimensions required to preserve 90% of variance is',dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- With the required number of dimensions found, we can then set number of dimensions to `dim` and run PCA again.\n",
    "\n",
    "- With the number of dimensions set to `dim`, we can then calculate the required accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot explained variance ratio with number of dimensions\n",
    "\n",
    "- An alternative option is to plot the explained variance as a function of the number of dimensions.\n",
    "\n",
    "- In the plot, we should look for an elbow where the explained variance stops growing fast.\n",
    "\n",
    "- This can be thought of as the intrinsic dimensionality of the dataset.\n",
    "\n",
    "- Now, I will plot cumulative explained variance ratio with number of components to show how variance ratio varies with number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlim(0,14,1)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "The above plot shows that almost 90% of variance is explained by the first 12 components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "-\tIn this kernel, I have discussed Principal Component Analysis – the most popular dimensionality reduction technique.\n",
    "-\tI have demonstrated PCA implementation with Logistic Regression on the adult dataset.\n",
    "-\tI found the maximum accuracy with the first 12 features and it is found to be 0.8227.\n",
    "-\tAs expected, the number of dimensions required to preserve 90 % of variance is found to be 12.\n",
    "-\tFinally, I plot the explained variance ratio with number of dimensions. The graph confirms that approximately 90% of variance is explained by the first 12 components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "The ideas and concepts in this kernel are taken from the following book.\n",
    "\n",
    "- Hands on Machine Learning with Scikit-Learn and Tensorflow by Aurelien Geron."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
